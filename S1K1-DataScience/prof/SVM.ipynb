{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine\n",
    "\n",
    "This exercise focuses on support vector machines for separating artificial data into two classes.\n",
    "\n",
    "The artificial data will be generated using methods from the scikit-learn package ```make_blobs``` and ```make circles```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn.datasets.samples_generator'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Workspace\\DataScience\\S1K1-DataScience\\prof\\SVM.ipynb Cell 2\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Workspace/DataScience/S1K1-DataScience/prof/SVM.ipynb#W1sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcolors\u001b[39;00m \u001b[39mimport\u001b[39;00m ListedColormap\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/DataScience/S1K1-DataScience/prof/SVM.ipynb#W1sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#make artificial datasets\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Workspace/DataScience/S1K1-DataScience/prof/SVM.ipynb#W1sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamples_generator\u001b[39;00m \u001b[39mimport\u001b[39;00m make_blobs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/DataScience/S1K1-DataScience/prof/SVM.ipynb#W1sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdatasets\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msamples_generator\u001b[39;00m \u001b[39mimport\u001b[39;00m make_circles\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Workspace/DataScience/S1K1-DataScience/prof/SVM.ipynb#W1sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#The SVM\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn.datasets.samples_generator'"
     ]
    }
   ],
   "source": [
    "# Necessary imports\n",
    "\n",
    "%pylab inline\n",
    "# large figures\n",
    "rcParams['figure.figsize'] = 8, 6\n",
    "\n",
    "#plotting in 3D\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "#make artificial datasets\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from sklearn.datasets.samples_generator import make_circles\n",
    "\n",
    "#The SVM\n",
    "from sklearn.svm import SVC  \n",
    "\n",
    "# Create color maps for the visualisation of the decision areas of the SVM\n",
    "cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])\n",
    "cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualisation\n",
    "\n",
    "The following function ```visualise_SVM``` illustrates the decision boundaries of the SVM, assuming a 2D problem (i.e. only 2 feature variables $x_1$ and $x_2$)\n",
    "It takes as input:\n",
    "   * the feature variables (e.g. from the training data x)\n",
    "   * the labels (e.g. the y values to predict)\n",
    "   * the instance of the SVM (to access the inner details of the trained SVM model)\n",
    "   \n",
    "The function shows\n",
    "   * the distribution of the data points of the feature variables\n",
    "   * the support vectors of the SVM\n",
    "   * the margin optimized during training\n",
    "   * the area which is associcated to e.g. class A and class B (according to the labels y) in the phase-space\n",
    "   \n",
    "The function is defined here at the top and is hence accessible throughout the exercise later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualise_SVM(x_data, y_data, clf):\n",
    "    # Plot the decision boundary by assigning a color to each point in the mesh [x_min, m_max]x[y_min, y_max].\n",
    "    x_min, x_max = x_data[:, 0].min() - 1, x_data[:, 0].max() + 1\n",
    "    y_min, y_max = x_data[:, 1].min() - 1, x_data[:, 1].max() + 1\n",
    "    x = np.linspace(x_min, x_max, 30)\n",
    "    y = np.linspace(y_min, y_max, 30)\n",
    "    yy,xx = np.meshgrid(y,x)\n",
    "    \n",
    "    #get the prediction from the classifier for the graphical illustration\n",
    "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    # get the distance of each point from the SVM decision boundary\n",
    "    P = np.zeros_like(xx)\n",
    "    for i, xi in enumerate(x):\n",
    "        for j, yj in enumerate(y):\n",
    "            P[i,j] = clf.decision_function(np.asarray([[xi,yj]]))\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.title(\"\")\n",
    "    plt.xlabel(\"$x_1$\", fontsize=25)\n",
    "    plt.ylabel(\"$x_2$\", fontsize=25)\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    #plot the SVM decision boundary\n",
    "    ax = plt.gca()\n",
    "    ax.contour(xx, yy, P, colors='k', levels=[-1, 0, 1], alpha=0.5, linestyles=['--', '-', '--'])\n",
    "    plt.scatter(x_data[:, 0], x_data[:, 1], c=y_data, cmap='bwr');\n",
    "\n",
    "    #highlight the SVM support vectors and margin violations (dark circles)\n",
    "    plt.scatter(clf.support_vectors_[:, 0], clf.support_vectors_[:, 1], s=150, marker='o', facecolors='black', alpha=0.5);\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear separable case\n",
    "\n",
    "First we generate 2 \"blobs\" of data which are well separated (```cluster_std=1.0```). This problem should be easily solvable with a simple SVM.\n",
    "\n",
    "Use the function ```make_blobs``` to generate features (x) and labels (y) in the following way:\n",
    "```x_train, y_train = make_blobs(n_samples=500, centers=2, n_features=2, random_state=2, cluster_std=1.0)```\n",
    "and visualise the resulting data using a scatter plot of the variables $x_1$ against $x_2$. Use a colour to indicate which label (y) each data-point belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: fix the random_state to make the figure reproducible\n",
    "\n",
    "x_train, y_train = make_blobs(n_samples=500, centers=2, n_features=2, random_state=2, cluster_std=1.0)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap='bwr');\n",
    "plt.title(\"\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=25)\n",
    "plt.ylabel(\"$x_2$\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now get an instance of the SVM with a linear kernel, train the SVM and use the function ```visualise_SVM``` to inspect the result.\n",
    "\n",
    "Address the following questions:\n",
    "   * Are the two \"blobs\" separated well?\n",
    "   * Are the support vectors OK?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear non-separable case\n",
    "\n",
    "Now increase the standard variation and create new data until the two blobs are no longer easily separabel.\n",
    "How does the SVM behave now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = make_blobs(n_samples=500, centers=2, n_features=2, random_state=2, cluster_std=3.0)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap='bwr');\n",
    "plt.title(\"\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=25)\n",
    "plt.ylabel(\"$x_2$\", fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-linear case\n",
    "\n",
    "In this part we generate which isn't linearly separable using the ```make_circles``` method. In this case, the two classes are generated such that they each form a circle and the two circles are inside each other as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data which aren't linearly separabel\n",
    "x_train, y_train = make_circles(100, factor=.1, noise=.1)\n",
    "plt.scatter(x_train[:, 0], x_train[:, 1], c=y_train, cmap='bwr');\n",
    "plt.title(\"\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=25)\n",
    "plt.ylabel(\"$x_2$\", fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try a linear SVM and visualise the result. Can the linear SVM separate these data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='linear')\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with polynomial kernel\n",
    "\n",
    "Next, try a polynomial kernel for the SVM. \n",
    "Start with the default values and see if the SVM is able to separate the data. \n",
    "Then, vary the following parameters:\n",
    "   * The kernel coefficient (```gamma```)\n",
    "   * The degree of the polynomial (```degree```)\n",
    "Can the SVM be make to classify the data correctly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='poly', gamma='auto')\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', gamma = 1)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', gamma = 0.5)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', gamma = 2.0)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', degree = 5.0, gamma='auto')\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='poly', degree = 2.0, gamma=2.)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM with sigmoid kernel\n",
    "\n",
    "Repeat the above exercise using a sigmoid as the kernel for the SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='sigmoid', gamma='auto')\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='sigmoid', gamma=0.5)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='sigmoid', gamma = 2.0)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Radial basis functions\n",
    "\n",
    "One \"trick\" used in SVMs is to transform the such that the new data become linearly separabel. One could for example go to a higher dimensional space where the data is located differently.\n",
    "\n",
    "Visualising the data in 3 dimensions (instead of 2) just shows that the data points are located on a plane in the phase-space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.abs(x_train).sum()\n",
    "\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(x_train[:, 0], x_train[:, 1], r, c=y_train, s=50, cmap='bwr')\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.set_xlabel('$x_1$', fontsize=25)\n",
    "ax.set_ylabel('$x_2$', fontsize=25)\n",
    "ax.set_zlabel('r', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, each data-point can now be modified to make use of the extra (higher) dimension.\n",
    "This can be done e.g. using radial basis function where the value of the function depends on the distance to the origin. This may allow to pull the classes of the data apart to be become separabel.\n",
    "An example is shown below for the circular data, where each data-point is assigned an additional value in the $r$ direction according to $r = e^{-\\vec{x}^2}$\n",
    "\n",
    "This concept is not limited to 2 or 3 dimensions (although it becomes difficult or impossible to visualise this in higher dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = np.exp(-(x_train)**2).sum(1)\n",
    "ax = plt.subplot(projection='3d')\n",
    "ax.scatter3D(x_train[:, 0], x_train[:, 1], r, c=y_train, s=50, cmap='bwr')\n",
    "ax.view_init(elev=30, azim=30)\n",
    "ax.set_xlabel('$x_1$', fontsize=25)\n",
    "ax.set_ylabel('$x_2$', fontsize=25)\n",
    "ax.set_zlabel('r', fontsize=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"blue\" and the \"red\" classes can now be separated easily, e.g. by a plane around $r=1.8$. Then all blue data are below the plane and all red data are above.\n",
    "\n",
    "Use a SVM with a radial basis function as kernel and see if the circular data can be separated this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clf = SVC(kernel='rbf', gamma='auto')\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experiment with the kernel coefficient (```gamma```) and observe how the results change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma = 1.0)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma = 0.5)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(kernel='rbf', gamma = 2.0)\n",
    "clf.fit(x_train, y_train)\n",
    "visualise_SVM(x_train,y_train,clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
